{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings  \n",
    "with warnings.catch_warnings():  \n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "    from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from pandas import read_csv, DataFrame, concat\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from keras.utils import plot_model\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "months = {'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, \n",
    "          'June': 6, 'July': 7, 'August': 8, 'September': 9, 'October': 10,\n",
    "         'November': 11, 'December': 12}\n",
    "\n",
    "def encode(l):\n",
    "    encoded = []\n",
    "    for m in l:\n",
    "        for key, value in months.items():\n",
    "            if key == m:\n",
    "                encoded.append(value)\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def parse(x):\n",
    "    return datetime.strptime(x, '%m/%d/%Y')\n",
    "\n",
    "forecast_length = 200 \n",
    "seed = 0\n",
    "name = 'suppdemand_march_' + str(seed)\n",
    "tf.set_random_seed(seed)\n",
    "shift = 8\n",
    "# Extend calendar to fit forecast_length\n",
    "add_weeks = math.ceil((forecast_length+shift)/5)\n",
    "\n",
    "\n",
    "data_set = pd.read_csv('march2020clean.csv', sep =',', date_parser = parse)\n",
    "\n",
    "    \n",
    "    \n",
    "# Data Loading\n",
    "data_set = pd.read_csv('march2020clean.csv', sep =',', date_parser = parse)\n",
    "data_set = data_set.iloc[:data_set['totalSoybeanMealSupply'].last_valid_index(),:] # Truncate dataset to those where every data is available \n",
    "'''\n",
    "'closePrice','openPrice','highPrice','lowPrice','totalSoybeanMealSupply','totalSoybeanMealDemand','soybeanOilSupply','soybeanOilDemand','sunflowerSeedPrice','canolaPrice','peanutsPrice','flaxseedPrice','soybeanOilPrice','cottonseedOilPrice','sunflowerseedOilPrice','canolaOilPrice','peanutOilPrice','cornOilPrice','soybeanMealPrice','cottonseedmealPrice','sunflowerseedMealPrice','linseedMealPrice'\n",
    "'canolaOilPrice', 'soybeanOilPrice', 'soybeanMealPrice', 'cottonseedmealPrice', 'sunflowerseedMealPrice'\n",
    "'''\n",
    "target_cols = ['closePrice','canolaOilPrice', 'soybeanOilPrice', 'soybeanMealPrice', 'cottonseedmealPrice', 'sunflowerseedMealPrice', 'Month']\n",
    "data_set['Month'] = encode(data_set['Month'])\n",
    "dates = data_set['date'].tolist()\n",
    "data_set = data_set.loc[:,target_cols]\n",
    "\n",
    "\n",
    "for i in range(len(dates)):\n",
    "    dates[i] = parse(dates[i])\n",
    "    \n",
    "last_week = dates[-5:]\n",
    "\n",
    "for i in range(add_weeks):\n",
    "    dates += [x + timedelta(days = 7) for x in last_week]\n",
    "    last_week = dates[-5:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = data_set.values\n",
    "values = values.astype('float32')\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 319, 7)\n",
      "(1, 319, 7)\n"
     ]
    }
   ],
   "source": [
    "def series_to_supervised(data, seq_length, y_col):\n",
    "    X = [] \n",
    "    Y = []\n",
    "    for time in range(len(data)-seq_length):\n",
    "        X += [data[time:time+seq_length]]\n",
    "        Y += [data[time+seq_length][y_col]]\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "\n",
    "data_x, data_y = scaled[:-shift], scaled[shift:]\n",
    "separation = int(len(data_x)*0.8)\n",
    "train_x, train_y = data_x[:separation], data_y[:separation]\n",
    "test_x, test_y = data_x[separation:], data_y[separation:]\n",
    "train_x = train_x.reshape(1, train_x.shape[0], train_x.shape[1])\n",
    "train_y = train_y.reshape(1, train_y.shape[0], train_y.shape[1])\n",
    "test_x = test_x.reshape(1, test_x.shape[0], test_x.shape[1])\n",
    "test_y = test_y.reshape(1, test_y.shape[0], test_y.shape[1])\n",
    "data_x = data_x.reshape(1, data_x.shape[0], data_x.shape[1])\n",
    "data_y = data_y.reshape(1, data_y.shape[0], data_y.shape[1])\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [1,269,7] vs. [1,269,4]\n\t [[{{node loss_5/lstm_45_loss/mean_squared_error/SquaredDifference}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-855bccfb9daf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_mse_warmup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# plot history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [1,269,7] vs. [1,269,4]\n\t [[{{node loss_5/lstm_45_loss/mean_squared_error/SquaredDifference}}]]"
     ]
    }
   ],
   "source": [
    "# design network\n",
    "def loss_mse_warmup(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error between y_true and y_pred,\n",
    "    but ignore the beginning \"warmup\" part of the sequences.\n",
    "    \n",
    "    y_true is the desired output.\n",
    "    y_pred is the model's output.\n",
    "    \"\"\"\n",
    "    warmup_steps = 50\n",
    "    # The shape of both input tensors are:\n",
    "    # [batch_size, sequence_length, num_y_signals].\n",
    "\n",
    "    # Ignore the \"warmup\" parts of the sequences\n",
    "    # by taking slices of the tensors.\n",
    "    y_true_slice = y_true[:, warmup_steps:, :]\n",
    "    y_pred_slice = y_pred[:, warmup_steps:, :]\n",
    "\n",
    "    # These sliced tensors both have this shape:\n",
    "    # [batch_size, sequence_length - warmup_steps, num_y_signals]\n",
    "\n",
    "    # Calculate the MSE loss for each value in these tensors.\n",
    "    # This outputs a 3-rank tensor of the same shape.\n",
    "    loss = tf.losses.mean_squared_error(labels=y_true_slice,\n",
    "                                        predictions=y_pred_slice)\n",
    "\n",
    "    # Keras may reduce this across the first axis (the batch)\n",
    "    # but the semantics are unclear, so to be sure we use\n",
    "    # the loss across the entire tensor, we reduce it to a\n",
    "    # single scalar with the mean function.\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss_mean\n",
    "\n",
    "regressor = Sequential()\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, stateful = False, input_shape = (None, train_x.shape[2])))\n",
    "regressor.add(LSTM(units = train_x.shape[2], return_sequences = True, stateful = False))\n",
    "regressor.compile(optimizer = RMSprop(lr=1e-3), loss = loss_mse_warmup)\n",
    "\n",
    "history = regressor.fit(train_x, train_y, epochs = 3000, verbose=0)\n",
    "\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "# pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "# print(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newModel = Sequential()\n",
    "newModel.add(LSTM(units = 50, return_sequences = True, stateful = True, batch_input_shape = (1, None, train_x.shape[2])))\n",
    "newModel.add(LSTM(units = train_x.shape[2], return_sequences = False, stateful = True))\n",
    "\n",
    "newModel.set_weights(regressor.get_weights())\n",
    "\n",
    "forecastFromSelf = np.empty((1, train_x.shape[1] + forecast_length, train_x.shape[2]))\n",
    "forecastData = np.empty((1, train_x.shape[1] + forecast_length, train_x.shape[2]))\n",
    "forecastData[:,:train_x.shape[1], :] = train_x[:,:,:]\n",
    "\n",
    "transformed_dates = scaler.fit_transform(np.asarray([x.month for x in dates]).reshape(len(dates),-1))\n",
    "for i in range(train_x.shape[1]+forecast_length):\n",
    "    input_data = forecastData[:,i:i+1,:]\n",
    "    forecastFromSelf[:,i:i+1,:] = newModel.predict(forecastData[:,i:i+1,:])\n",
    "    forecastFromSelf[:,i,-1] = transformed_dates[i+shift,0] # Replace prediction with what's already known\n",
    "    output_data = forecastFromSelf[:,i:i+1,:]\n",
    "    if i + shift >= train_x.shape[1] and i + shift < forecastData.shape[1]:\n",
    "        forecastData[:,i+shift:i+shift+1,:] = forecastFromSelf[:,i:i+1,:]\n",
    "\n",
    "y_hat = scaler.inverse_transform(forecastFromSelf.reshape((forecastFromSelf.shape[1],forecastFromSelf.shape[2])))\n",
    "a = pyplot.figure()\n",
    "for i in range(y_hat.shape[1]):\n",
    "    ax = a.add_subplot()\n",
    "    b = data_y\n",
    "    b = b.reshape(b.shape[1], b.shape[2])\n",
    "    b = scaler.inverse_transform(b)\n",
    "    pyplot.plot(b[:,i], label='Actual')\n",
    "    pyplot.plot(y_hat[:,i], label='Forecast')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data \n",
    "regressor.save(open('pickles/{}.p'.format(name), 'wb'))\n",
    "a.savefig(fname='./data/{}.png'.format(name))\n",
    "    \n",
    "dates = dates[:y_hat.shape[0]]\n",
    "output_dict = {'Date':dates}\n",
    "for i, col in enumerate(target_cols):\n",
    "    output_dict[col] = y_hat[:,i]\n",
    "\n",
    "\n",
    "output_df = DataFrame(output_dict, columns=['Date']+target_cols)\n",
    "output_df.to_csv('./data/{}.csv'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
